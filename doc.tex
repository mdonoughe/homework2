\documentclass[11pt]{article}
\usepackage{geometry}
\geometry{letterpaper}
\usepackage[in]{fullpage}
\usepackage{times}
\usepackage{url}

\begin{document}
This time I wrote a fast neural network in C and GLSL.

The C code runs the neural network on the test set and learns. Inputs are presented in random order. The learn rate is 0.0001. There is a momentum term that starts at 0.95 and adjusts between 0.5 and 0.99 using the method described by Kun-Long Ho, Yuan-Yih Hsu, and Chien-Chuen Yang in ``Short Term Load Forecasting Using a Multilayer Neural Network with an Adaptive Learning Algorithm''(\url{http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=141697}).

The C code also generates some GLSL code that runs the network on the GPU. Every pixel is classified and colorized four times, and the average is used for the final color value. This provides a somewhat real-time preview of the network as it learns. This program will not run without a graphics card driver supporting OpenGL 2.1 or later with \texttt{EXT\_bindable\_uniform}. This program should not be run with a high number of neurons on old hardware or the computer may freeze up or become so unresponsive that it is unusable or think that the GPU has crashed and try to reset it.

Because of the realtime preview, this program does not read spirtest.dat. The points in spirtest.dat are surely included in the generated image, so there is no need and it would just make things cluttered and hard to see.

There are three modes available at compile time. The precompiled versions and the provided build scripts are all set to TURBO. This is controlled by preprocessor definitions.
\begin{description}
\item[SLOW] Look at one input and update the weights once between drawing frames. This mode does not support the automatic learning rate adjustment, and the epoch numbers written to the console will be incorrect.
\item[normal(no special flag)] Look at all the inputs and update the weights once between drawing frames.
\item[TURBO] Launch a background thread that continously looks at all the inputs and updates the weights as fast as possible.
\end{description}

Run from the command line like \texttt{homework2.exe -f ..\textbackslash spiral.dat 200} to make a 200-node MLP, or \texttt{homework2.exe -f ..\textbackslash spiral.dat -r 150} to make a 150-node RBF. The \texttt{-r} option must come after the \texttt{-f}, and the \texttt{-r} option must be followed by a layer size. There are a few other training sets provided.

The program will spit out the fragment shader generated by shaderbuilder.c to the console and then print out lines with the epoch number followed by the mean square error between 0.0 and 1.0 as the network learns. The preview window will show all the points in the training set, colored by their classification, and all the other pixels will be colored according to the current network's classification of that point.

Higher learning rates on the spiral data cause the network to generalize and learn very poorly. The network comes out more as an explosion than a spiral. Lower learning rates are just slower.

Less hidden neurons causes the network to be unable to correctly learn the training set because the clusters overlap. Too many hidden neurons eventually causes the network to get gaps in the spiral, more like a series of dots.

The spread of the activation function in this implementation is determined by the variance of the points in the cluster plus a small constant. Changing this doesn't seem to have much impact on the network because the weight of the cluster is adjusted by the neural network, and causes the cluster's effective spread to change as the network learns.

Adding an additional hidden layer seems to make the network run slower(both in terms of CPU time and epochs) without actually doing anything for it.

Changing the MLP parameters gives the same results as changing the RBF parameters.

The RBF learns much faster and can work with a smaller set of neurons, by changing the problem from ``what makes a point belong to this class?'' to ``what cluster is this point close to and what class does it have?''

Assigning clusters through K-means seems like it can have problems with high contrast areas that have low density because the classes of the points are ignored while clustering. More clusters are assigned in high density areas, but a low density area where the two classes are close together can result in a cluster being made up of multiple classes and being incapable of training successfully.
\end{document}
